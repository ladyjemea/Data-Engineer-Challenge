# Architecture

## Overview

This architecture ingests, processes, and visualizes real-time cryptocurrency market data to provide actionable insights. The system is designed to be scalable, fault-tolerant, and capable of processing high-throughput data from sources like Coinbase.

## Components
### 1. Data Ingestion Layer
   - **Apache Kafka**: Acts as the primary data ingestion system. Data is published to the `crypto_prices` topic, allowing downstream systems to consume it.
   - **Purpose**: Provides high-throughput, fault-tolerant data streaming.
   - **Justification**: Kafka’s scalability and durability make it ideal for handling real-time data.

### 2. Data Processing Layer
   - **Apache Beam on Google Cloud Dataflow**: A unified data processing engine that applies transformations to the data.
   - **Purpose**: Processes data from Kafka, calculating insights such as the highest bid, lowest ask, max spread, and moving average.
   - **Justification**: Google Dataflow provides managed scalability, allowing the pipeline to scale based on data load.

### 3. Data Storage Layer
   - **PostgreSQL**: A relational database to store processed metrics and historical data.
   - **Schema**:
     - `crypto_metrics` table stores fields like `pair`, `bid`, `ask`, `mid_price`, `spread`, `highest_bid`, `lowest_ask`, `max_spread`, and `moving_avg`.
   - **Purpose**: Ensures structured and durable storage for further analysis and historical reference.
   - **Justification**: PostgreSQL’s relational model supports complex queries and joins, ideal for analytical needs.

### 4. Visualization and Analysis Layer
   - **Python Matplotlib**:
     - **Python Matplotlib**: Used for local development and testing. It generates time series plots for key metrics.
   - **Purpose**: Allows the finance team to visualize key metrics and make informed decisions.
   - **Justification**:Matplotlib is a lightweight alternative to Looker for local data visualization.

### 5. Monitoring, Logging, and Error Handling Layer
   - **Centralized Logging (Pipeline.log)**:
     - A centralized log file, `pipeline.log`, captures all events, including data ingestion, processing, database interactions, and errors.
     - Logs provide detailed information on data flow, performance, and any issues encountered.
   - **Error Handling**:
     - Critical sections of the code, such as data ingestion and database operations, use `try-except` blocks with logging to capture and report errors without interrupting the entire application.
   - Retries are implemented in functions such as `save_metrics_to_db` to    handle transient errors gracefully.
     - The logging system helps monitor application health and troubleshoot issues by providing insight into data flows and system failures.
   - **Purpose**: Ensures system reliability by tracking performance, logging errors, and providing insights for troubleshooting.
   - **Justification**: Centralized logging and error handling improve visibility into the pipeline’s health, making it easier to detect and resolve issues.


## Workflow Summary
1. **Data Generation**:
   - Cryptocurrency data is either fetched from Coinbase (in production) or generated by `data_stream.py` (for testing).

2. **Data Ingestion**:
   - Data is published to the `crypto_prices` Kafka topic, enabling real-time data streaming.

3. **Data Processing**:
   - Dataflow/Apache Beam pulls data from Kafka, processes it, calculates metrics, and stores them in Google Cloud SQL.

4. **Storage and Analysis**:
   - Processed data is stored in PostgreSQL. Metrics are visualized through  Matplotlib for decision-making.

5. **Monitoring and Logging**:
   - A centralized log file, `pipeline.log`, captures all events and logs provide detailed information on data flow, performance, and any issues encountered.


## Diagram
    +---------------------------------------+
    |    Stream Processing Architecture     |
    |                                       |
    |  +---------------------------------+  |
    |  |          Data Ingestion         |  |
    |  |          (Apache Kafka)         |  |
    |  |                                 |  |
    |  | Receives and streams real-time  |  |
    |  | data. Published to a topic      |  |
    |  | called crypto_prices enabling   |  |
    |  | continuous data streaming       |  |
    |  +---------------------------------+  |
    |                   |                   |
    |                   |                   |
    |                   v                   |
    |  +---------------------------------+  |
    |  |         Processing Layer        |  |                     
    |  |          (Apache Beam)          |  |
    |  |                                 |  |
    |  | Kafka streams data to this layer|  |
    |  | where matrices are calculated   |  |
    |  | in real time and prepared for   |  |
    |  | storage                         |  |
    |  +---------------------------------+  |
    |                   |                   |
    |                   v                   |
    |  +---------------------------------+  |
    |  |         Storage layer           |  |
    |  |        (PostgreSQL DB)          |  |
    |  |                                 |  |
    |  | Stores processed metrices,      |  |
    |  | enabling historical analysis    |  |
    |  | and tracking.                   |  |
    |  +---------------------------------+  |
    |                   |                   |
    |                   v                   |
    |  +-----------------------------------+|
    |  |    Vizualization and Insight    |  |
    |  |       (Python Matplotlib)       |  |
    |  |                                 |  |
    |  | Visualises the matrices stored  |  |
    |  | in Postgres. Provides insights  |  |
    |  | through the series plots and    |  |
    |  | helps the finance  team make    |  |
    |  | decisions                       |  |
    |  +---------------------------------+  |
    |                   |                   |
    |                   v                   |
    |  +---------------------------------+  |
    |  |     Monitoring and  Logging     |  |
    |  |         (Local Logging)         |  |
    |  |                                 |  |
    |  | Logs errors and provides alerts |  |
    |  | for issues in the pipeline.     |  |
    |  | Monitors Kafka, the processing  |  |
    |  | layer, Postgres and the         |  |
    |  | vizualization tool to ensure    |  |
    |  | smooth operations and log       |  |
    |  | anomalies                       |  |
    |  +---------------------------------+  |
    +---------------------------------------+


## Future Considerations

- **Scaling**: Deploying multiple consumers in Kafka, partitioning topics for scalability.
- **Data Quality**: Implementing additional data validation to handle edge cases from data sources.
- **Advanced Forecasting**: Adding more sophisticated forecasting methods in Beam to provide predictive insights.